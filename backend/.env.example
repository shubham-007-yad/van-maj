# Copy to .env and fill values (or export as environment variables)
OPENAI_API_KEY=your_openai_api_key_here
# If using Ollama (local), ensure the daemon is running on http://localhost:11434
# and you have pulled a model, e.g.:  ollama pull llama3.1